# HDBSCAN Clustering Pipeline - Complete Configuration
# Version 2.0 - Cumulative Training with Versioned Storage
# Single comprehensive configuration file

# ===================================================================
# GLOBAL SETTINGS
# ===================================================================
global:
  project_name: "HDBSCAN Incident Classification Pipeline"
  version: "2.0.0"
  environment: "production"  # development, staging, production

# ===================================================================
# AZURE CONFIGURATION
# ===================================================================
azure:
  # OpenAI Configuration
  openai:
    endpoint: ${AZURE_OPENAI_ENDPOINT}
    api_key: ${OPENAI_API_KEY}
    api_version: ${AZURE_OPENAI_API_VERSION}
    deployment_name: ${AZURE_OPENAI_CHAT_DEPLOYMENT_NAME}
      # Embeddings configuration
    embedding_endpoint: ${AZURE_OPENAI_EMBEDDING_ENDPOINT}
    embedding_api_version: ${AZURE_OPENAI_EMBEDDING_API_VERSION}
    embedding_key: ${AZURE_OPENAI_EMBEDDING_KEY}
    embedding_model: ${AZURE_OPENAI_EMBEDDING_MODEL}

  # Blob Storage Configuration - Versioned Model Storage
  blob_storage:
    connection_string: ${BLOB_CONNECTION_STRING}
    container_name: "hdbscan-models"
    structure:
      # Versioned directory structure: {tech_center_slug}/{version}/
      models: "{tech_center_slug}/{version}/"
      logs: "logs/{date}/"
      backups: "backups/{version}/"

# ===================================================================
# BIGQUERY CONFIGURATION
# ===================================================================
bigquery:
  project_id: "enterprise-dashboardnp-cd35"
  service_account_key_path: ${SERVICE_ACCOUNT_KEY_PATH}
    # Source tables
  tables:
    # Raw data sources
    incidents: ${INCIDENT_TABLE}
    team_services: ${TEAM_SERVICES_TABLE}
    problems: ${PROBLEM_TABLE}
      # Source table for training data (updated for your specific table)
    incident_source: "enterprise-dashboardnp-cd35.bigquery_datasets_hone_srv_dev.oa_snow_incident_mgmt_srv_dev"
    raw_incidents: "enterprise-dashboardnp-cd35.bigquery_datasets_hone_srv_dev.oa_snow_incident_mgmt_srv_dev"

    # Pipeline tables - HYBRID STORAGE ARCHITECTURE
    preprocessed_incidents: "enterprise-dashboardnp-cd35.bigquery_datasets_hone_srv_dev.preprocessed_incidents"
    # ^ Contains: embeddings, summaries, metadata (HIGH cost but necessary)

    # Versioned training results (NO embeddings for cost optimization)
    training_results_template: "clustering_predictions_{version}_{hash}"
    training_data: "enterprise-dashboardnp-cd35.bigquery_datasets_hone_srv_dev.training_data"
    # ^ Example: clustering_predictions_2025_q2_789abc12

    # Live predictions (NO embeddings)
    predictions: "enterprise-dashboardnp-cd35.bigquery_datasets_hone_srv_dev.incident_predictions"    # Model tracking
    model_registry: "enterprise-dashboardnp-cd35.bigquery_datasets_hone_srv_dev.model_registry"
    
    # NEW: Training cycle metadata for 6-month cycles across all tech centers
    training_cycle_metadata: "enterprise-dashboardnp-cd35.bigquery_datasets_hone_srv_dev.training_cycle_metadata"

    # Operational tables (BigQuery)
    training_logs: "enterprise-dashboardnp-cd35.bigquery_datasets_hone_srv_dev.training_logs"
    watermarks: "enterprise-dashboardnp-cd35.bigquery_datasets_hone_srv_dev.preprocessing_watermarks"
  # SQL Query templates to eliminate hardcoded queries
  queries:
    # Simplified query template that uses parameters provided by the preprocessing pipeline
    incident_data_for_preprocessing: |
      SELECT
        number,
        sys_created_on,
        description,
        short_description,
        business_service
      FROM `{source_table}`
      WHERE sys_created_on >= '{start_time}'
      AND sys_created_on <= '{end_time}'
      ORDER BY sys_created_on ASC    # Training data query with tech center filtering
    training_data_window: |
      SELECT
        p.number,
        t.TechCenter as tech_center,
        p.sys_created_on,
        p.combined_incidents_summary,
        p.embedding
      FROM `{preprocessed_incidents_table}` p
      LEFT JOIN `{team_services_table}` t
      ON t.business_service = p.business_service
      WHERE p.sys_created_on >= '{start_date}'
      AND p.sys_created_on <= '{end_date}'
      AND t.TechCenter = '{tech_center}'
      AND p.embedding IS NOT NULL
      AND ARRAY_LENGTH(p.embedding) > 0
      ORDER BY p.sys_created_on DESC

    model_registry_insert: |
      INSERT INTO `{table}` (model_version, tech_center, model_type, training_data_start,
                            training_data_end, blob_path, created_timestamp, model_params)
      VALUES (@model_version, @tech_center, @model_type, @training_data_start,
              @training_data_end, @blob_path, @created_timestamp, @model_params)    # Training cycle metadata query for model lookup
    get_latest_model_for_tech_center: |
      SELECT 
        umap_artifact_path,
        hdbscan_artifact_path,
        metadata_artifact_path,
        model_version,
        model_hash,
        cluster_results_table,
        embeddings_source_table
      FROM `{training_cycle_metadata_table}`
      WHERE tech_center = '{tech_center}'
        AND training_status = 'SUCCESS'
      ORDER BY training_completed_date DESC
      LIMIT 1

    # Insert new training cycle metadata
    training_cycle_metadata_insert: |
      INSERT INTO `{table}` (
        training_cycle_id, tech_center, tech_center_slug, training_month, training_year,
        training_completed_date, umap_artifact_path, hdbscan_artifact_path, 
        metadata_artifact_path, cluster_results_table, embeddings_source_table,
        model_version, model_hash, clusters_count, domains_count, incidents_count,
        silhouette_score, noise_ratio, training_status, error_message, created_timestamp
      ) VALUES (
        @training_cycle_id, @tech_center, @tech_center_slug, @training_month, @training_year,
        @training_completed_date, @umap_artifact_path, @hdbscan_artifact_path,
        @metadata_artifact_path, @cluster_results_table, @embeddings_source_table,
        @model_version, @model_hash, @clusters_count, @domains_count, @incidents_count,
        @silhouette_score, @noise_ratio, @training_status, @error_message, @created_timestamp
      )

    # Simple query for getting latest sys_created_on timestamp for preprocessing
    get_watermark_for_preprocessing: |
      SELECT MAX(sys_created_on) as last_processed_timestamp
      FROM `{table_id}`
      {tech_center_filter}

  # Schema definitions for table creation
  schemas:
    model_registry:
      - {name: "model_version", type: "STRING", mode: "REQUIRED"}
      - {name: "tech_center", type: "STRING", mode: "REQUIRED"}
      - {name: "model_type", type: "STRING", mode: "REQUIRED"}
      - {name: "training_data_start", type: "DATE", mode: "REQUIRED"}
      - {name: "training_data_end", type: "DATE", mode: "REQUIRED"}
      - {name: "blob_path", type: "STRING", mode: "REQUIRED"}
      - {name: "created_timestamp", type: "TIMESTAMP", mode: "REQUIRED"}
      - {name: "model_params", type: "JSON", mode: "NULLABLE"}
      - {name: "cluster_count", type: "INTEGER", mode: "NULLABLE"}
      - {name: "silhouette_score", type: "FLOAT", mode: "NULLABLE"}
      # Schema for preprocessed_incidents table (simplified output)
    preprocessed_incidents:
      - {name: "number", type: "STRING", mode: "REQUIRED"}
      - {name: "sys_created_on", type: "TIMESTAMP", mode: "REQUIRED"}
      - {name: "combined_incidents_summary", type: "STRING", mode: "NULLABLE"}
      - {name: "embedding", type: "FLOAT", mode: "REPEATED"}  # Fixed: type should be FLOAT, mode REPEATED
      - {name: "created_timestamp", type: "TIMESTAMP", mode: "REQUIRED"}
      - {name: "processing_version", type: "STRING", mode: "REQUIRED"}

    training_data:
      - {name: "incident_number", type: "STRING", mode: "REQUIRED"}
      - {name: "tech_center", type: "STRING", mode: "REQUIRED"}
      - {name: "description_summary", type: "STRING", mode: "NULLABLE"}
      - {name: "embedding", type: "REPEATED", mode: "NULLABLE", fields: [{name: "value", type: "FLOAT"}]}
      - {name: "training_version", type: "STRING", mode: "REQUIRED"}
      - {name: "created_timestamp", type: "TIMESTAMP", mode: "REQUIRED"}    # Schema for versioned training results tables (cluster_results_{version}_{hash})
    cluster_results:
      - {name: "number", type: "STRING", mode: "REQUIRED"}
      - {name: "sys_created_on", type: "TIMESTAMP", mode: "REQUIRED"}
      - {name: "combined_incidents_summary", type: "STRING", mode: "NULLABLE"}
      - {name: "tech_center", type: "STRING", mode: "REQUIRED"}
      - {name: "cluster_id", type: "INTEGER", mode: "REQUIRED"}
      - {name: "cluster_label", type: "STRING", mode: "NULLABLE"}
      - {name: "cluster_description", type: "STRING", mode: "NULLABLE"}
      - {name: "domain_id", type: "INTEGER", mode: "NULLABLE"}
      - {name: "domain_name", type: "STRING", mode: "NULLABLE"}
      - {name: "umap_x", type: "FLOAT", mode: "NULLABLE"}
      - {name: "umap_y", type: "FLOAT", mode: "NULLABLE"}
      - {name: "model_version", type: "STRING", mode: "REQUIRED"}
      - {name: "model_hash", type: "STRING", mode: "REQUIRED"}
      - {name: "prediction_timestamp", type: "TIMESTAMP", mode: "REQUIRED"}

    # ADD SCHEMAS FOR OPERATIONAL TABLES
    training_logs:
      - {name: "run_id", type: "STRING", mode: "REQUIRED"}
      - {name: "timestamp", type: "TIMESTAMP", mode: "REQUIRED"}
      - {name: "pipeline_stage", type: "STRING", mode: "NULLABLE"}
      - {name: "tech_center", type: "STRING", mode: "NULLABLE"}
      - {name: "model_version", type: "STRING", mode: "NULLABLE"}
      - {name: "log_level", type: "STRING", mode: "REQUIRED"}
      - {name: "message", type: "STRING", mode: "REQUIRED"}
      - {name: "details", type: "JSON", mode: "NULLABLE"}

    watermarks:
      - {name: "pipeline_name", type: "STRING", mode: "REQUIRED"} # e.g., 'preprocessing', 'prediction'
      - {name: "tech_center", type: "STRING", mode: "REQUIRED"}
      - {name: "last_processed_timestamp", type: "TIMESTAMP", mode: "NULLABLE"}
      - {name: "last_processed_id", type: "STRING", mode: "NULLABLE"}
      - {name: "updated_at", type: "TIMESTAMP", mode: "REQUIRED"}

    # Schema for versioned training results tables (clustering_predictions_{version}_{hash})
    versioned_training_results:
      - {name: "incident_number", type: "STRING", mode: "REQUIRED"}
      - {name: "cluster_id", type: "INTEGER", mode: "REQUIRED"}
      - {name: "cluster_label", type: "STRING", mode: "NULLABLE"}
      - {name: "domain_id", type: "INTEGER", mode: "NULLABLE"}
      - {name: "domain_name", type: "STRING", mode: "NULLABLE"}
      - {name: "umap_x", type: "FLOAT", mode: "NULLABLE"}
      - {name: "umap_y", type: "FLOAT", mode: "NULLABLE"}
      - {name: "tech_center", type: "STRING", mode: "REQUIRED"}
      - {name: "model_version", type: "STRING", mode: "REQUIRED"}
      - {name: "confidence_score", type: "FLOAT", mode: "NULLABLE"}
      - {name: "created_timestamp", type: "TIMESTAMP", mode: "REQUIRED"}

    # Schema for live incident predictions
    incident_predictions:
      - {name: "incident_id", type: "STRING", mode: "REQUIRED"}
      - {name: "predicted_cluster_id", type: "INTEGER", mode: "REQUIRED"}
      - {name: "predicted_cluster_label", type: "STRING", mode: "NULLABLE"}
      - {name: "confidence_score", type: "FLOAT", mode: "NULLABLE"}
      - {name: "predicted_domain_id", type: "INTEGER", mode: "NULLABLE"}
      - {name: "predicted_domain_name", type: "STRING", mode: "NULLABLE"}
      - {name: "tech_center", type: "STRING", mode: "REQUIRED"}
      - {name: "prediction_timestamp", type: "TIMESTAMP", mode: "REQUIRED"}
      - {name: "model_table_used", type: "STRING", mode: "NULLABLE"}
      - {name: "blob_model_path", type: "STRING", mode: "NULLABLE"}

    preprocessing_watermarks:
      - {name: "preprocessed_rows", type: "INTEGER", mode: "REQUIRED"}
      - {name: "time_trigger", type: "TIMESTAMP", mode: "REQUIRED"}
      - {name: "run_details", type: "JSON", mode: "NULLABLE"}    # NEW: Training cycle metadata schema (optimized - no duplicate embeddings)
    training_cycle_metadata:
      - {name: "training_cycle_id", type: "STRING", mode: "REQUIRED"}  # e.g., "2024_q4"
      - {name: "tech_center", type: "STRING", mode: "REQUIRED"}
      - {name: "tech_center_slug", type: "STRING", mode: "REQUIRED"}
      - {name: "training_month", type: "INTEGER", mode: "REQUIRED"}  # 6 or 12
      - {name: "training_year", type: "INTEGER", mode: "REQUIRED"}
      - {name: "training_completed_date", type: "TIMESTAMP", mode: "REQUIRED"}
      - {name: "umap_artifact_path", type: "STRING", mode: "REQUIRED"}
      - {name: "hdbscan_artifact_path", type: "STRING", mode: "REQUIRED"}
      - {name: "metadata_artifact_path", type: "STRING", mode: "REQUIRED"}
      - {name: "cluster_results_table", type: "STRING", mode: "REQUIRED"}
      - {name: "embeddings_source_table", type: "STRING", mode: "REQUIRED"}  # Reference to BigQuery table
      - {name: "model_version", type: "STRING", mode: "REQUIRED"}
      - {name: "model_hash", type: "STRING", mode: "REQUIRED"}
      - {name: "clusters_count", type: "INTEGER", mode: "NULLABLE"}
      - {name: "domains_count", type: "INTEGER", mode: "NULLABLE"}
      - {name: "incidents_count", type: "INTEGER", mode: "NULLABLE"}
      - {name: "silhouette_score", type: "FLOAT", mode: "NULLABLE"}
      - {name: "noise_ratio", type: "FLOAT", mode: "NULLABLE"}
      - {name: "training_status", type: "STRING", mode: "REQUIRED"}  # SUCCESS, FAILED, RUNNING
      - {name: "error_message", type: "STRING", mode: "NULLABLE"}
      - {name: "created_timestamp", type: "TIMESTAMP", mode: "REQUIRED"}

# ===================================================================
# PREPROCESSING CONFIGURATION
# ===================================================================
preprocessing:
  # Text processing configuration
  text_columns_to_process: ["description", "short_description", "business_service"]
  text_column_for_summary_input: "combined_text_for_summary"
  summary_column_name: "combined_incidents_summary"

  # Processing version for tracking
  processing_version: "v2.0.0"

  # Tech center mapping (business_service -> tech_center)
  tech_center_mapping:
    "Data Analytics": "BT-TC-Data Analytics"
    "Network Operations": "BT-TC-Network Operations"
    "Security Operations": "BT-TC-Security Operations"
    "Infrastructure Services": "BT-TC-Infrastructure Services"
    "Product Development": "BT-TC-Product Development Engineering"
    "Cloud Services": "BT-TC-Cloud Services"
    "Enterprise Applications": "BT-TC-Enterprise Applications"
    # Add more mappings as needed
  default_tech_center: "BT-TC-General"

  # Text cleaning settings
  clean_text:
    remove_special_characters: true
    remove_emails: true
    remove_urls: true
    normalize_whitespace: true
    min_text_length: 10
    # Summarization settings
  summarization:
    enabled: true
    max_summary_length: 30  # Maximum words in summary
    max_input_chars: 400000  # 100K tokens ≈ 400K characters (generous limit)
    chunk_size_chars: 50000  # 12.5K tokens per chunk for very large texts
    summary_prompt_template: |
      Summarize the following incident information in exactly 30 words or less.
      Focus on: what issues occurred and which application/system was affected.
      Format: [Issue description] affecting [Application/System name].

      Incident Details:
      {combined_text}

      30-word Summary:
    model_name: "gpt-35-turbo"
    max_retries: 3
    batch_size: 50
    delay_between_batches: 0.5  # Add delay between batches to reduce rate limiting
    max_parallel_requests: 5    # Limit parallel requests to Azure OpenAI

# ===================================================================
# TRAINING CONFIGURATION - CUMULATIVE APPROACH
# ===================================================================
training:
  # Semi-annual cumulative training (NEW ARCHITECTURE)
  schedule:
    frequency: "semi_annual"  # Every 6 months instead of quarterly
    months: [6, 12]  # June and December
    training_window_months: 24  # Always use 24-month datasets

  # Training parameters optimized for large cumulative datasets
  parameters:
    # UMAP settings
    umap:
      n_components: 2
      n_neighbors: 15
      min_dist: 0.1
      metric: "cosine"
      random_state: 42

    # HDBSCAN settings
    hdbscan:
      min_cluster_size: 25  # Increased for larger datasets
      min_samples: 5
      cluster_selection_epsilon: 0.1
      metric: "euclidean"
      cluster_selection_method: "eom"

    # Domain grouping
    domain_grouping:
      max_domains_per_tech_center: 20
      min_cluster_size_for_domain: 5
      similarity_threshold: 0.7

  # Model versioning
  versioning:
    version_format: "{year}_q{quarter}"  # e.g., "2025_q2"
    hash_algorithm: "sha256"
    hash_length: 8

  # Processing settings
  processing:
    parallel_tech_centers: true
    max_workers: 4
    timeout_hours: 6  # Longer for large cumulative datasets
    batch_size: 1000
    max_incidents_per_training: 100000

# ===================================================================
# PREDICTION CONFIGURATION - REAL-TIME CLASSIFICATION
# ===================================================================
prediction:
  # Execution schedule
  schedule:
    frequency_minutes: 120  # Every 2 hours
    batch_size: 500
    timeout_minutes: 30

  # Model management
  model_loading:
    cache_models: true
    cache_ttl_hours: 24
    version_strategy: "latest"  # Use latest available version
    fallback_version: "2024_q4"  # Fallback if latest fails
    preload_models: true

  # Prediction parameters
  parameters:
    min_confidence_score: 0.3
    high_confidence_threshold: 0.8
    max_distance_to_cluster: 2.0
    enable_domain_prediction: true

# ===================================================================
# CLUSTERING CONFIGURATION
# ===================================================================
clustering:
  # Core clustering parameters (same as training.parameters for consistency)
  umap:
    n_components: 2
    n_neighbors: 15
    min_dist: 0.1
    metric: "cosine"
    random_state: 42

  hdbscan:
    min_cluster_size: 25
    min_samples: 5
    metric: "euclidean"
    cluster_selection_epsilon: 0.1

  # Quality thresholds
  quality:
    min_silhouette_score: 0.15
    min_clusters: 3
    max_noise_ratio: 0.30

# ===================================================================
# TECH CENTERS CONFIGURATION
# ===================================================================
tech_centers:
  # All tech centers with consistent format
  primary:
    - name: "BT-TC-AI, Analytics & Data"
      slug: "bt-tc-ai-analytics-data"
      min_incidents: 500

    - name: "BT-TC-Core Platforms"
      slug: "bt-tc-core-platforms"
      min_incidents: 750

    - name: "BT-TC-Global Tech Services"
      slug: "bt-tc-global-tech-services"
      min_incidents: 800

    - name: "BT-EF-Global CTO"
      slug: "bt-ef-global-cto"
      min_incidents: 600

    - name: "BT-TC-Product Development & Engineering"
      slug: "bt-tc-product-development-engineering"
      min_incidents: 700

    - name: "BT-TC-Cyber Security"
      slug: "bt-tc-cyber-security"
      min_incidents: 600

    - name: "BT-TC-Finance & Controlling"
      slug: "bt-tc-finance-controlling"
      min_incidents: 500

    - name: "BT-EF-People"
      slug: "bt-ef-people"
      min_incidents: 450

    - name: "BT-IF-Central People Organisation"
      slug: "bt-if-central-people-organisation"
      min_incidents: 400

    - name: "BT-IF-Sales"
      slug: "bt-if-sales"
      min_incidents: 450

    - name: "BT-IF-Portfolio Brands"
      slug: "bt-if-portfolio-brands"
      min_incidents: 400

    - name: "BT-IF-Product Organization"
      slug: "bt-if-product-organization"
      min_incidents: 500

    - name: "BT-IF-Logistics"
      slug: "bt-if-logistics"
      min_incidents: 550

    - name: "H&M Group"
      slug: "hm-group"
      min_incidents: 500

    - name: "Central People Organization"
      slug: "central-people-organization"
      min_incidents: 450

# ===================================================================
# COST OPTIMIZATION - 50% REDUCTION STRATEGY
# ===================================================================
cost_optimization:
  # Storage optimization
  storage:
    separate_embeddings: true     # Keep embeddings separate from results
    bigquery_minimal: true        # Only essential data in BigQuery
    models_in_blob: true          # Store large models in cheap blob storage
    use_partitioning: true        # Partition tables by date
    use_clustering: true          # Cluster tables by tech_center

  # Training optimization
  training:
    semi_annual_frequency: true   # Reduce from quarterly to semi-annual
    parallel_processing: true     # Optimize compute efficiency
    cleanup_old_models: true      # Keep only last 3 versions

  # Query optimization
  queries:
    query_cache: true
    slot_limits: true
    maximum_bytes_billed: 10737418240  # 10 GB limit

# ===================================================================
# MONITORING & ALERTING
# ===================================================================
monitoring:
  # Key metrics to track
  metrics:
    training:
      - "training_duration"
      - "cluster_count"
      - "silhouette_score"
      - "noise_ratio"
      - "model_size_mb"

    prediction:
      - "prediction_latency"
      - "confidence_distribution"
      - "model_version_usage"
      - "error_rate"

    cost:
      - "bigquery_slot_usage"
      - "blob_storage_size"
      - "monthly_cost_usd"

  # Alerts
  alerts:
    training_failure: true
    low_quality_clusters: true
    high_cost: true
    prediction_errors: true

# ===================================================================
# PERFORMANCE & SCALING
# ===================================================================
performance:
  # Memory management for large 24-month datasets
  memory:
    max_memory_usage_gb: 16.0
    enable_memory_monitoring: true
    batch_processing: true

  # Parallel processing
  parallel:
    enable_multiprocessing: true
    max_workers: 4
    chunk_size: 1000

  # Caching
  caching:
    enable_caching: true
    cache_models: true
    cache_ttl_hours: 24

# ===================================================================
# SECURITY & GOVERNANCE
# ===================================================================
security:
  # Data protection
  encryption_at_rest: true
  encryption_in_transit: true
  audit_logging: true

  # Access control
  rbac_enabled: true
  service_account_rotation: true

  # Compliance
  data_retention_days: 730  # 2 years
  pii_handling: "anonymized"

# ===================================================================
# LOGGING CONFIGURATION
# ===================================================================
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

  handlers:
    console:
      enabled: true
      level: "INFO"

    file:
      enabled: true
      path: "logs/"
      filename: "pipeline_{timestamp}.log"
      max_size: "100MB"
      backup_count: 5

# ===================================================================
# ENVIRONMENT-SPECIFIC OVERRIDES
# ===================================================================
environments:
  development:
    global:
      log_level: "DEBUG"
    bigquery:
      dataset: "hdbscan_pipeline_dev"
      maximum_bytes_billed: 1073741824  # 1 GB limit
    blob_storage:
      container_name: "hdbscan-models-dev"
    training:
      max_incidents_per_training: 10000  # Smaller datasets for dev

  staging:
    bigquery:
      dataset: "hdbscan_pipeline_staging"
      maximum_bytes_billed: 5368709120  # 5 GB limit
    blob_storage:
      container_name: "hdbscan-models-staging"

  production:
    bigquery:
      dataset: "hdbscan_pipeline"
      maximum_bytes_billed: 10737418240  # 10 GB limit
    blob_storage:
      container_name: "hdbscan-models"

# ===================================================================
# EXAMPLE USAGE COMMANDS
# ===================================================================
# Training: python training_orchestrator.py --year 2025 --quarter q2
# Prediction: python prediction_pipeline.py --batch-size 500
# Analysis: python cluster_analysis.py --tech-center "BT-TC-Data Analytics"

# ===================================================================
# ARCHITECTURE BENEFITS
# ===================================================================
# ✅ 50% cost reduction through hybrid storage
# ✅ Semi-annual training reduces operational overhead
# ✅ 24-month cumulative windows for pattern stability
# ✅ Versioned model deployment with rollback capability
# Azure Configuration
azure:
  blob_storage:
    connection_string: "${BLOB_CONNECTION_STRING}"
    container_name: "hdbscan-models"
    
  openai:
    endpoint: "${AZURE_OPENAI_ENDPOINT}"
    api_key: "${OPENAI_API_KEY}"
    api_version: "${AZURE_OPENAI_API_VERSION}"
    deployment_name: "${AZURE_OPENAI_CHAT_DEPLOYMENT_NAME}"

# Model Storage Configuration  
model_storage:
  local_path: "models"
  blob_storage_enabled: true

# ✅ Enterprise-grade monitoring and security