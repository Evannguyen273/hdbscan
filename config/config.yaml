# Main Configuration for HDBSCAN Clustering Pipeline
# Compatible with both original pipeline.py and enhanced main.py
# Updated to match .env file configuration

# Azure OpenAI Configuration
azure:
  openai:
    endpoint: ${AZURE_OPENAI_ENDPOINT}
    api_key: ${OPENAI_API_KEY}
    api_version: ${AZURE_OPENAI_API_VERSION}
    deployment_name: ${AZURE_OPENAI_CHAT_DEPLOYMENT_NAME}
    
    # Embeddings configuration
    embedding_endpoint: ${AZURE_OPENAI_EMBEDDING_ENDPOINT}
    embedding_api_version: ${AZURE_OPENAI_EMBEDDING_API_VERSION}
    embedding_key: ${AZURE_OPENAI_EMBEDDING_KEY}
    embedding_model: ${AZURE_OPENAI_EMBEDDING_MODEL}

# BigQuery Configuration
bigquery:
  project_id: "enterprise-dashboardnp-cd35"
  
  # Service account configuration
  service_account_key_path: ${SERVICE_ACCOUNT_KEY_PATH}
    # Table configurations (from .env)
  tables:
    team_services: ${TEAM_SERVICES_TABLE}
    incidents: ${INCIDENT_TABLE}
    raw_incidents: ${INCIDENT_TABLE}  # Alias for compatibility
    problems: ${PROBLEM_TABLE}
    
    # Data pipeline tables - optimized for cost efficiency
    preprocessed_incidents: "enterprise-dashboardnp-cd35.preprocessing_pipeline.preprocessed_incidents"
    # ^ Contains: embeddings, combined_incidents_summary, preprocessing metadata
    # ^ Storage: HIGH (embeddings are expensive to store)
    
    clustering_predictions: "enterprise-dashboardnp-cd35.bigquery_datasets_home_srv_dev.clustering_predictions"
    # ^ Contains: cluster results, domain grouping, UMAP coordinates (NO embeddings)
    # ^ Storage: LOW (analytical results only, references preprocessed_incidents for embeddings)
    
    preprocessing_watermarks: "enterprise-dashboardnp-cd35.preprocessing_pipeline.preprocessing_watermarks"
    
    # Legacy support for original pipeline
    embeddings_table: "enterprise-dashboardnp-cd35.preprocessing_pipeline.preprocessed_incidents"
    results_table: "enterprise-dashboardnp-cd35.bigquery_datasets_home_srv_dev.clustering_predictions"

# Azure Blob Storage Configuration
blob_storage:
  connection_string: ${BLOB_CONNECTION_STRING}
  container_name: "prediction-artifacts"
  
  # Path templates
  structure:
    models: "models/{tech_center}/{year}/{quarter}"
    predictions: "predictions/{tech_center}/{year}/{quarter}"
    monitoring: "monitoring/{tech_center}"
    preprocessing: "preprocessing/{tech_center}/{year}/{quarter}"

# Clustering Configuration
clustering:
  # HDBSCAN parameters
  hdbscan:
    min_cluster_size: 25
    min_samples: 5
    metric: "euclidean"
    cluster_selection_epsilon: 0.0
  
  # Domain grouping configuration (from Untitled-1 approach)
  domain_grouping:
    enabled: true
    max_domains: 20
    min_incidents_per_domain: 5
    optimization_metric: "combined"  # silhouette, calinski_harabasz, combined
    hierarchical_linkage: "ward"     # ward, complete, average, single
    
  # UMAP parameters  
  umap:
    n_components: 50
    n_neighbors: 100
    min_dist: 0.1
    metric: "euclidean"
    random_state: 42
  
  # Embedding configuration - Pure semantic as requested
  embedding:
    weights:
      entity: 0.0      # Disabled as per requirements
      action: 0.0      # Disabled as per requirements
      semantic: 1.0    # Pure semantic embeddings only
    batch_size: 25
    
  # Domain grouping
  max_domains: 20
  min_incidents_per_domain: 5

# Pipeline Configuration
pipeline:  # Output settings
  save_to_local: true
  result_path: "./results"  # Windows-compatible path
  use_checkpointing: true
  
  # Processing settings
  parallel_training: true
  max_workers: 4
  
  # Schedule configuration
  training_schedule:
    quarters: ["q1", "q2", "q3", "q4"]
    months:
      q1: [1, 2, 3]
      q2: [4, 5, 6]
      q3: [7, 8, 9]
      q4: [10, 11, 12]
  
  # Pipeline frequencies
  preprocessing:
    frequency_minutes: 60    # Every hour
    batch_size: 1000
    
  prediction:
    frequency_minutes: 120   # Every 2 hours
    batch_size: 500
    
  # Retry configuration
  max_retries: 3
  retry_delay_seconds: 30

# Tech Centers Configuration (15 total)
tech_centers:
  - "BT-TC-Product Development & Engineering"
  - "BT-TC-Infrastructure Services"
  - "BT-TC-Network Operations"
  - "BT-TC-Security Operations"
  - "BT-TC-Cloud Services"
  - "BT-TC-Data Analytics"
  - "BT-TC-Enterprise Applications"
  - "BT-TC-Field Services"
  - "BT-TC-Customer Support"
  - "BT-TC-Quality Assurance"
  - "BT-TC-DevOps Engineering"
  - "BT-TC-Business Intelligence"
  - "BT-TC-Systems Integration"
  - "BT-TC-Mobile Solutions"
  - "BT-TC-Compliance & Governance"

# Logging Configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  
  handlers:
    console:
      enabled: true
      level: "INFO"
      
    file:
      enabled: true
      path: "logs/"
      filename: "pipeline_{timestamp}.log"
      max_size: "100MB"
      backup_count: 5
      
    azure_monitor:
      enabled: false
      workspace_id: "${AZURE_WORKSPACE_ID}"

# Notification Configuration
notifications:
  email:
    enabled: false
    smtp_server: "smtp.company.com"
    smtp_port: 587
    from: "pipeline@company.com"
    recipients:
      - "admin@company.com"
      - "team@company.com"
    username: "${EMAIL_USERNAME}"
    password: "${EMAIL_PASSWORD}"
    
  teams:
    enabled: false
    webhook_url: "${TEAMS_WEBHOOK_URL}"
    
  azure_monitor:
    enabled: false
    alert_rule_id: "${AZURE_ALERT_RULE_ID}"

# Model Configuration
model:
  # Model versioning
  versioning:
    enabled: true
    strategy: "quarterly"  # quarterly, monthly, manual
    
  # Model storage
  storage:
    local_path: "models/"
    blob_enabled: true
    retention_quarters: 4  # Keep last 4 quarters
    
  # Model validation
  validation:
    min_silhouette_score: 0.15
    min_clusters: 2
    max_noise_ratio: 0.80

# Performance Configuration
performance:
  # Memory management
  max_memory_usage_gb: 8.0
  enable_memory_monitoring: true
  
  # Parallel processing
  enable_multiprocessing: true
  chunk_size: 1000
  
  # Caching
  enable_caching: true
  cache_embeddings: true
  cache_ttl_hours: 24

# Legacy Support (for original pipeline.py)
legacy:
  # Default table IDs for backward compatibility
  default_embeddings_table: "enterprise-dashboardnp-cd35.preprocessing_pipeline.preprocessed_incidents"
  default_results_table: "enterprise-dashboardnp-cd35.bigquery_datasets_hone_srv_dev.clustering_predictions"
  
  # Stage configuration
  stages:
    embedding: true
    clustering: true
    analysis: true
    saving: true
    
  # Default parameters
  default_dataset_name: "hdbscan_clustering"
  default_write_disposition: "WRITE_APPEND"

# Development/Testing Configuration
development:
  # Testing settings
  test_mode: false
  sample_size: 1000
  mock_embeddings: false
  
  # Debug settings
  verbose_logging: false
  save_intermediate_results: true
  skip_expensive_operations: false